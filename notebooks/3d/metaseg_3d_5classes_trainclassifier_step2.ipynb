{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f074b3ca",
   "metadata": {},
   "source": [
    "## MetaSeg 3D segmentation STEP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017cf88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b64f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c09b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(422)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path as osp\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, os.path as osp\n",
    "import torch\n",
    "import alpine\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a21517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../modules')\n",
    "sys.path.append(\"../../\")\n",
    "from learner import INRMetaLearner\n",
    "import dataloaders\n",
    "\n",
    "import models\n",
    "import loss_functions\n",
    "import metrics\n",
    "import utils\n",
    "import vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac53be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db3be1",
   "metadata": {},
   "source": [
    "## Set dataset paths here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f376626",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"\" ## ENTER DATSET PATH HERE!!!!!!!!!!!\n",
    "config_file = \"../../config/oasis_splits_3d.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33abd6a",
   "metadata": {},
   "source": [
    "## Load learned weights / initialization from step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = \"./dumps/WEIGHT_FILE_FROM_STEP_1\" ## ENTER PATH TO BEST WEIGHTS FROM STEP 1 HERE!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a835d12",
   "metadata": {},
   "source": [
    "### Set saving flag as True to export saveed feature vectors for finetuning the dataset. This step was done to simply make the model run on a smaller server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FEATURE_VECS = False\n",
    "USE_SAVED_FEATURE_VECS_TO_TRAIN_CLF = not SAVE_FEATURE_VECS\n",
    "\n",
    "SAVE_PATHS = \"./dumps/intermediate_vectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab981594",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_data = json.load(open(config_file, 'r'))\n",
    "train_files = files_data['train']\n",
    "val_files = files_data['val']\n",
    "test_files = files_data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check overlap for any samples.\n",
    "print(len(train_files), len(val_files), len(test_files))\n",
    "set_train = set([x['img'] for x in train_files])\n",
    "set_val = set([x['img'] for x in val_files])\n",
    "set_test = set([x['img'] for x in test_files])\n",
    "\n",
    "if len(set_train.intersection(set_val)) > 0 or  len(set_train.intersection(set_test)) > 0 or  len(set_val.intersection(set_test)) > 0:\n",
    "    print(\"WARNING: OVERLAPPING DATA SPLITS\")\n",
    "else:\n",
    "    print(\"No overlap in data splits. GOOD TO GO!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b598ecb",
   "metadata": {},
   "source": [
    "## Some hyperparameters for the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a2d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "INNER_STEPS = 2\n",
    "RANDOM_AUGMENT = False\n",
    "TEST_RUN_STEPS = VAL_STEPS  = 300 # for full res. Hyperparameters. you can use values like T=100, 300. For 3D you need to let it render for more number of iterations unlike simple 2D images.\n",
    "SKIP_PIXELS = 2\n",
    "VAL_META_STEPS = 100\n",
    "OUTER_LOOP_ITERATIONS =  5000  # 5000\n",
    "NUM_CLASSES = 4\n",
    "NUM_CLASSES_AND_ONE = NUM_CLASSES + 1\n",
    "# RES = (160, 192, 224)\n",
    "\n",
    "RES = (160,160,200)\n",
    "VAL_RES = RES = [160//SKIP_PIXELS, 160//SKIP_PIXELS, 200//SKIP_PIXELS]\n",
    "\n",
    "NORMALIZE_FEATURES = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad57de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlin = 'siren'\n",
    "inr_config = {\"in_features\":3, \"out_features\": 1, \"hidden_features\": 256, \"hidden_layers\": 4, }#\"first_omega_0\":200.0, 'hidden_omega_0':200.0} \n",
    "segmentation_config = {'hidden_features':[256,],#[128, 64],\n",
    "                         'output_features' : NUM_CLASSES_AND_ONE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1129098",
   "metadata": {},
   "outputs": [],
   "source": [
    "inr_seg_model = models.SirenSegINR(\n",
    "    inr_type='siren',\n",
    "    inr_config=inr_config,\n",
    "    segmentation_config=segmentation_config,\n",
    "    normalize_features=NORMALIZE_FEATURES,\n",
    "    ).float().cuda()\n",
    "\n",
    "\n",
    "weights_from_metalearning = torch.load(weights_file)\n",
    "inr_seg_model_wts = weights_from_metalearning['inr_seg_model']\n",
    "best_inr_weights = weights_from_metalearning['best_inr_weights']\n",
    "best_classifier_weights = weights_from_metalearning['best_classifier_weights']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_tmp = alpine.utils.coords.get_coords2d(RES[0], RES[1]).float().cuda()[None,...]\n",
    "print(coords_tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a91adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataloaders.TorchMRI3D_Dataloader(json_file=config_file, mode='train', resolution=RES, coords=coords_tmp, config={'augment':RANDOM_AUGMENT}, num_classes=NUM_CLASSES, skip_pixels=SKIP_PIXELS)\n",
    "val_ds = dataloaders.TorchMRI3D_Dataloader(json_file=config_file, mode='val', resolution=RES, coords=coords_tmp, config={'augment':RANDOM_AUGMENT, 'N_samples':10}, num_classes=NUM_CLASSES, skip_pixels=SKIP_PIXELS)\n",
    "test_ds = dataloaders.TorchMRI3D_Dataloader(json_file=config_file, mode='test', resolution=RES, coords=coords_tmp, config={'augment':RANDOM_AUGMENT}, num_classes=NUM_CLASSES, skip_pixels=SKIP_PIXELS)\n",
    "print(len(train_ds), len(val_ds), len(test_ds))\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=1, shuffle=False)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed911370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_clf_features = []\n",
    "val_clf_features = []\n",
    "\n",
    "if SAVE_FEATURE_VECS:\n",
    "\n",
    "    pbar_train = tqdm(enumerate(train_dl), total=len(train_dl), position=0)\n",
    "    for train_ix, train_data in pbar_train:\n",
    "        _tmp_save_check =  osp.join(SAVE_PATHS,\"train\" , f\"train_{train_ix}.pth\")\n",
    "        if osp.isfile(_tmp_save_check):\n",
    "            continue\n",
    "        \n",
    "        inr_model_train = models.INR(**inr_config).float().cuda()\n",
    "        inr_model_train.load_state_dict({k.replace(\"inr.\",\"\"): v.clone().detach() for k,v in deepcopy(best_inr_weights).items()})\n",
    "        inr_model_train.compile()\n",
    "\n",
    "        train_img = train_data['img'].float().cuda()\n",
    "        train_seg = train_data['seg'].float().cuda()\n",
    "        train_coords = train_data['coords'].float().cuda()\n",
    "\n",
    "        inr_model_train.fit(train_coords, train_img, epochs=TEST_RUN_STEPS, disable_tqdm=True) \n",
    "        train_img_output, train_img_features = inr_model_train.forward_w_features(train_coords)\n",
    "\n",
    "        data_dt = {\n",
    "            'seg': train_seg.detach().clone().cpu().numpy(),\n",
    "            'img': train_img_output.detach().clone().cpu().numpy(),\n",
    "            'features': train_img_features[-2].detach().clone().cpu().numpy() # input to classifier\n",
    "        }\n",
    "\n",
    "        if SAVE_FEATURE_VECS:\n",
    "            os.makedirs(osp.join(SAVE_PATHS,\"train\"), exist_ok=True)\n",
    "            torch.save(data_dt, osp.join(SAVE_PATHS,\"train\" , f\"train_{train_ix}.pth\"),pickle_protocol=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_FEATURE_VECS:\n",
    "\n",
    "    pbar_val = tqdm(enumerate(val_dl), total=len(val_dl), position=0)\n",
    "    for val_ix, val_data in pbar_val:\n",
    "        _tmp_save_check =  osp.join(SAVE_PATHS,\"val\" , f\"val_{train_ix}.pth\")\n",
    "        if osp.isfile(_tmp_save_check):\n",
    "            continue\n",
    "        \n",
    "        inr_model_val = models.INR(**inr_config).float().cuda()\n",
    "        inr_model_val.load_state_dict({k.replace(\"inr.\",\"\"): v.clone().detach() for k,v in deepcopy(best_inr_weights).items()})\n",
    "        inr_model_val.compile()\n",
    "\n",
    "        val_img = val_data['img'].float().cuda()\n",
    "        val_seg = val_data['seg'].float().cuda()\n",
    "        val_coords = val_data['coords'].float().cuda()\n",
    "\n",
    "        inr_model_val.fit(val_coords, val_img, epochs=TEST_RUN_STEPS, disable_tqdm=True)\n",
    "        val_img_output, val_img_features = inr_model_val.forward_w_features(val_coords)\n",
    "\n",
    "        val_dt = {\n",
    "            'seg':val_seg.detach().clone().cpu().numpy(),\n",
    "            'img': val_img.detach().clone().cpu().numpy(),\n",
    "            'features': val_img_features[-2].detach().clone().cpu().numpy(),\n",
    "        }\n",
    "\n",
    "        if SAVE_FEATURE_VECS:\n",
    "            os.makedirs(osp.join(SAVE_PATHS, \"val\"), exist_ok=True)\n",
    "            torch.save(val_dt, osp.join(SAVE_PATHS,\"val\" , f\"val_{val_ix}.pth\"), pickle_protocol=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80832db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_FEATURE_VECS:\n",
    "\n",
    "    test_clf_features = True\n",
    "    test_features = []\n",
    "    if test_clf_features:\n",
    "        pbar_test = tqdm(enumerate(test_dl), total=len(test_dl), position=0)\n",
    "        for test_ix, test_data in pbar_test:\n",
    "            inr_model_test = models.INR(**inr_config).float().cuda()\n",
    "            inr_model_test.load_state_dict({k.replace(\"inr.\",\"\"): v.clone().detach() for k,v in deepcopy(best_inr_weights).items()})\n",
    "            inr_model_test.compile()\n",
    "\n",
    "            test_img = test_data['img'].float().cuda()\n",
    "            test_seg = test_data['seg'].float().cuda()\n",
    "            test_coords = test_data['coords'].float().cuda()\n",
    "\n",
    "            inr_model_test.fit(test_coords, test_img, epochs=TEST_RUN_STEPS, disable_tqdm=True)\n",
    "            test_img_output, test_img_features = inr_model_test.forward_w_features(test_coords)\n",
    "\n",
    "            test_dt = {\n",
    "                'seg':test_seg.detach().clone().cpu().numpy(),\n",
    "                'img': test_img.detach().clone().cpu().numpy(),\n",
    "                # 'coords' : val_coords.detach().clone().cpu().numpy(),\n",
    "                # 'resolution': val_data['resolution'],\n",
    "                'features': test_img_features[-2].detach().clone().cpu().numpy(),\n",
    "            }\n",
    "\n",
    "            os.makedirs(osp.join(SAVE_PATHS,\"test\"), exist_ok=True)\n",
    "            torch.save(test_dt, osp.join(SAVE_PATHS,\"test\" , f\"test_{test_ix}.pth\"), pickle_protocol=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SAVED_FEATURE_VECS_TO_TRAIN_CLF:\n",
    "    data_dir_feature_vecs = SAVE_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890dfe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_clf_features_ds = dataloaders.CLFFeature(data_dir_feature_vecs, mode='train')\n",
    "val_clf_ds = dataloaders.CLFFeature(data_dir_feature_vecs, mode='val')\n",
    "test_clf_ds = dataloaders.CLFFeature(data_dir_feature_vecs, mode='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_clf_dl = torch.utils.data.DataLoader(train_clf_features_ds, batch_size=1, shuffle=False, num_workers=8, pin_memory=True)\n",
    "val_clf_dl = torch.utils.data.DataLoader(val_clf_ds, batch_size=1, shuffle=False, num_workers=8, pin_memory=True)\n",
    "test_clf_dl = torch.utils.data.DataLoader(test_clf_ds, batch_size=1, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17860d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIER_FINETUNE_EPOCHS = 100_000 # until convergence. stop when you see acc no longer decrease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054fdf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORTANT: this step may have key mismatch based on how the model was saved. simply use the str.replace() function to match your saved keys to the model's named parameters\n",
    "\n",
    "\n",
    "classifier_model = deepcopy(inr_seg_model.segmentation_head)\n",
    "\n",
    "classifier_weights = deepcopy(best_classifier_weights)\n",
    "try:\n",
    "    classifier_model.load_state_dict(classifier_weights['final_clf_weights']) # check key, if final_clf_weights key does not exist, then just load classifier_weights as shown above.\n",
    "except:\n",
    "    classifier_weights = deepcopy({k.replace(\"segmentation_head.segmentation_head\",\"segmentation_head\"):v.clone().detach() for k,v in best_classifier_weights.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b279a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SAVED_FEATURE_VECS_TO_TRAIN_CLF:\n",
    "    LEARNING_RATE = 5e-5\n",
    "    FOCAL_LOSS_GAMMA = 3.0\n",
    "    ZERO_WT = 0.1\n",
    "\n",
    "    EXPERIMENT_NAME  = f\"gamma_{FOCAL_LOSS_GAMMA}_INR_300it_skip_pixels_{SKIP_PIXELS}_continue\"\n",
    "\n",
    "    classifier_opt = torch.optim.Adam(classifier_model.parameters(), lr=LEARNING_RATE)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(classifier_opt, CLASSIFIER_FINETUNE_EPOCHS, eta_min=1e-6)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.StepLR(classifier_opt, step_size=100, gamma=0.95)\n",
    "    print(classifier_model, list(classifier_weights.keys()), list(classifier_model.state_dict().keys()))\n",
    "\n",
    "    finetune_classifier_loss_fn = loss_functions.LossFunction({'focal_loss':loss_functions.FocalSemanticLoss(gamma=FOCAL_LOSS_GAMMA)})\n",
    "\n",
    "    final_classifier_weights = None\n",
    "    best_val_score = 1e7\n",
    "\n",
    "    pbar_epochs = tqdm(range(CLASSIFIER_FINETUNE_EPOCHS), position=0)\n",
    "    for epoch in pbar_epochs:\n",
    "        \n",
    "        avg_loss_per_set = 0.0\n",
    "\n",
    "        for train_ix, train_data in enumerate(train_clf_dl):\n",
    "            train_seg = train_data['seg'].float().cuda()\n",
    "            classifier_input = train_data['features'].float().cuda().squeeze(0).squeeze(0) # NC x F\n",
    "            if NORMALIZE_FEATURES:\n",
    "                classifier_input = nn.functional.normalize(classifier_input, dim=-1)\n",
    "\n",
    "            classifier_opt.zero_grad()\n",
    "            classifier_output = classifier_model(classifier_input)\n",
    "            classifier_output = classifier_output.unsqueeze(0).unsqueeze(0)\n",
    "            loss, loss_info = finetune_classifier_loss_fn({'output':{'segmentation_output':classifier_output}, 'seg':train_seg})\n",
    "            loss.backward()\n",
    "            classifier_opt.step()\n",
    "            # lr_scheduler.step()\n",
    "            avg_loss_per_set += float(loss.item())\n",
    "            # pbar.set_description(f\"Loss: {loss.item():.5f} ce={loss_info['ce_loss']:.5f}, PSNR = {psnr.item():.5f} Dice={loss_info['dice_loss']:.5f}\")\n",
    "        avg_loss_per_set /= len(train_clf_dl)\n",
    "        pbar_epochs.set_description(f\"Loss (clf): {avg_loss_per_set:.5f}. Best Val Loss(clf): {best_val_score:.5f}\")\n",
    "        pbar_epochs.refresh()\n",
    "\n",
    "        if epoch % VAL_META_STEPS == 0 and epoch > 0:\n",
    "            with torch.no_grad():\n",
    "                avg_val_loss = 0.0\n",
    "                for val_ix, val_data in enumerate(val_clf_dl):\n",
    "                    val_seg = val_data['seg'].float().cuda()\n",
    "                    classifier_val_input = val_data['features'].float().cuda().squeeze(0).squeeze(0) # NC x F\n",
    "                    if NORMALIZE_FEATURES:\n",
    "                        classifier_val_input = nn.functional.normalize(classifier_val_input, dim=-1)\n",
    "                    \n",
    "                    classifier_val_output = classifier_model(classifier_val_input)\n",
    "                    classifier_val_output = classifier_val_output.unsqueeze(0).unsqueeze(0)\n",
    "                    val_loss, val_loss_info = finetune_classifier_loss_fn({'output':{'segmentation_output':classifier_val_output}, 'seg':val_seg})\n",
    "\n",
    "                    avg_val_loss += float(val_loss.item())\n",
    "                avg_val_loss = avg_val_loss / len(val_clf_dl)\n",
    "                pbar_epochs.set_description(f\"Loss (clf): {avg_loss_per_set:.5f} Val Loss (clf): {avg_val_loss:.5f}\")\n",
    "                pbar_epochs.refresh()\n",
    "\n",
    "                if avg_val_loss < best_val_score:\n",
    "                    best_val_score = avg_val_loss\n",
    "                    final_classifier_weights = deepcopy(classifier_model.state_dict())\n",
    "                    tqdm.write(f'updated best val score to {best_val_score}')\n",
    "                    torch.save({'final_clf_weights':final_classifier_weights, 'focal_loss_gamma':FOCAL_LOSS_GAMMA, 'zero_wt':ZERO_WT}, \n",
    "                            f\"./dumps/weights_3d/classifierfinal_weights_LR_{LEARNING_RATE}_exp_{EXPERIMENT_NAME}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712c74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
